{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65827f6-ce61-497d-869f-8927fa358e61",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6461b2a3-364f-4b8b-80f2-48a2c27b4527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/viktor/jupyter/lib/python3.12/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /home/viktor/jupyter/lib/python3.12/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/viktor/jupyter/lib/python3.12/site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/viktor/jupyter/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/viktor/jupyter/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/viktor/jupyter/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/viktor/jupyter/lib/python3.12/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /home/viktor/jupyter/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/viktor/jupyter/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/viktor/jupyter/lib/python3.12/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/viktor/jupyter/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/viktor/jupyter/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/viktor/jupyter/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/viktor/jupyter/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/viktor/jupyter/lib/python3.12/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/viktor/jupyter/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/viktor/jupyter/lib/python3.12/site-packages (from requests->transformers) (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e7c7c5-620a-416e-a07e-ee3bf1265e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessor, LogitsProcessorList, GPT2LMHeadModel, GPT2Tokenizer, PrefixConstrainedLogitsProcessor\n",
    "from transformers.tokenization_utils import Trie\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "import torch\n",
    "# import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f22a32-c790-4ded-9706-5d5cd842b850",
   "metadata": {},
   "source": [
    "# Build Vocabulary\n",
    "Creating a set of words to work as a vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b7e57c-0f66-4bbf-8e14-dc0106ee958e",
   "metadata": {},
   "source": [
    "## Dolch word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073034b0-3f8f-4ce2-94ea-ddced30a134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_primer = [\"a\", \"and\", \"away\", \"big\", \"blue\", \"can\", \"come\", \"down\", \"find\", \"for\", \"funny\", \"go\", \"help\", \"here\", \"I\", \"in\", \"is\", \"it\", \"jump\", \"little\", \"look\", \"make\", \"me\", \"my\", \"not\", \"one\", \"play\", \"red\", \"run\", \"said\", \"see\", \"the\", \"three\", \"to\", \"two\", \"up\", \"we\", \"where\", \"yellow\", \"you\"]\n",
    "primer = [\"all\", \"am\", \"are\", \"at\", \"ate\", \"be\", \"black\", \"brown\", \"but\", \"came\", \"did\", \"do\", \"eat\", \"four\", \"get\", \"good\", \"have\", \"he\", \"into\", \"like\", \"must\", \"new\", \"no\", \"now\", \"on\", \"our\", \"out\", \"please\", \"pretty\", \"ran\", \"ride\", \"saw\", \"say\", \"she\", \"so\", \"soon\", \"that\", \"there\", \"they\", \"this\", \"too\", \"under\", \"want\", \"was\", \"well\", \"went\", \"what\", \"white\", \"who\", \"will\", \"with\", \"yes\"]\n",
    "grade_1 = [\"after\", \"again\", \"an\", \"any\", \"as\", \"ask\", \"by\", \"could\", \"every\", \"fly\", \"from\", \"give\", \"going\", \"had\", \"has\", \"her\", \"him\", \"his\", \"how\", \"just\", \"know\", \"let\", \"live\", \"may\", \"of\", \"old\", \"once\", \"open\", \"over\", \"put\", \"round\", \"some\", \"stop\", \"take\", \"thank\", \"them\", \"then\", \"think\", \"walk\", \"were\", \"when\"]\n",
    "grade_2 = [\"always\", \"around\", \"because\", \"been\", \"before\", \"best\", \"both\", \"buy\", \"call\", \"cold\", \"does\", \"don't\", \"fast\", \"first\", \"five\", \"found\", \"gave\", \"goes\", \"green\", \"its\", \"made\", \"many\", \"off\", \"or\", \"pull\", \"read\", \"right\", \"sing\", \"sit\", \"sleep\", \"tell\", \"their\", \"these\", \"those\", \"upon\", \"us\", \"use\", \"very\", \"wash\", \"which\", \"why\", \"wish\", \"work\", \"would\", \"write\", \"your\"]\n",
    "grade_3 = [\"about\", \"better\", \"bring\", \"carry\", \"clean\", \"cut\", \"done\", \"draw\", \"drink\", \"eight\", \"fall\", \"far\", \"full\", \"got\", \"grow\", \"hold\", \"hot\", \"hurt\", \"if\", \"keep\", \"kind\", \"laugh\", \"light\", \"long\", \"much\", \"myself\", \"never\", \"only\", \"own\", \"pick\", \"seven\", \"shall\", \"show\", \"six\", \"small\", \"start\", \"ten\", \"today\", \"together\", \"try\", \"warm\"]\n",
    "nouns = [\"apple\", \"baby\", \"back\", \"ball\", \"bear\", \"bed\", \"bell\", \"bird\", \"birthday\", \"boat\", \"box\", \"boy\", \"bread\", \"brother\", \"cake\", \"car\", \"cat\", \"chair\", \"chicken\", \"children\", \"Christmas\", \"coat\", \"corn\", \"cow\", \"day\", \"dog\", \"doll\", \"door\", \"duck\", \"egg\", \"eye\", \"farm\", \"farmer\", \"father\", \"feet\", \"fire\", \"fish\", \"floor\", \"flower\", \"game\", \"garden\", \"girl\", \"goat\", \"grass\", \"ground\", \"hand\", \"head\", \"hill\", \"home\", \"horse\", \"house\", \"kitty\", \"leg\", \"letter\", \"man\", \"men\", \"milk\", \"money\", \"morning\", \"mother\", \"name\", \"nest\", \"night\", \"paper\", \"party\", \"picture\", \"pig\", \"rabbit\", \"rain\", \"ring\", \"robin\", \"Santa Claus\", \"school\", \"seed\", \"sheep\", \"shoe\", \"sister\", \"snow\", \"song\", \"squirrel\", \"stick\", \"street\", \"sun\", \"table\", \"thing\", \"time\", \"top\", \"toy\", \"tree\", \"watch\", \"water\", \"way\", \"wind\", \"window\", \"woman\", \"women\", \"wood\"]\n",
    "\n",
    "dolch_word_list = pre_primer + primer + grade_1 + grade_2 + grade_3 + nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4224ad3-bf09-4240-92e2-6bcb2d4c2ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "vocabulary = dolch_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7947114c-9d11-4c3d-aa96-a0cecf5a3574",
   "metadata": {},
   "source": [
    "In order to avoid splitting the words into smaller tokens, we need a word level tokenizer. This is because otherwise out of vocabulary words may be produced using tokens from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b9780e-649f-4f59-b503-e782027b9502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option vocab\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer with an empty model\n",
    "tokenizer = Tokenizer(models.WordLevel())\n",
    "\n",
    "# Set up pre-tokenizer and decoder\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.decoder = decoders.WordPiece()\n",
    "\n",
    "# Define the trainer with your vocabulary\n",
    "trainer = trainers.WordLevelTrainer(vocab=dolch_word_list + ['<pad>', '<unk>', '.', ',', '!', '?', ';', ':'])\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(dolch_word_list, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "181a2782-9fec-4d66-a2f8-59f2259d9da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load GPT-2 tokenizer and model\n",
    "# model_name = 'gpt2'\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e67112c-b395-48a2-9620-5220f63726c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Your vocabulary\n",
    "# vocabulary = dolch_word_list\n",
    "\n",
    "# num_beams = 5\n",
    "\n",
    "# # Tokenize vocabulary words and build the trie\n",
    "# vocabulary_token_sequences = []\n",
    "# for word in vocabulary:\n",
    "#     tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "#     vocabulary_token_sequences.append(tokens)\n",
    "\n",
    "# trie = Trie()\n",
    "# for token_seq in vocabulary_token_sequences:\n",
    "#     trie.add(tuple(token_seq))\n",
    "\n",
    "# # Initialize the constrained logits processor\n",
    "# logits_processor = LogitsProcessorList()\n",
    "# logits_processor.append(PrefixConstrainedLogitsProcessor(\n",
    "#     prefix_allowed_tokens_fn=lambda batch_id, input_ids: [\n",
    "#         token_seq[len(input_ids)] for token_seq in vocabulary_token_sequences\n",
    "#         if len(token_seq) > len(input_ids) and token_seq[:len(input_ids)] == tuple(input_ids.tolist())\n",
    "#     ] if any(\n",
    "#         len(token_seq) > len(input_ids) and token_seq[:len(input_ids)] == tuple(input_ids.tolist())\n",
    "#         for token_seq in vocabulary_token_sequences\n",
    "#     ) else list(range(len(tokenizer)))  # Fallback to allow any token if no matches found\n",
    "#     ,\n",
    "#     num_beams=num_beams\n",
    "# ))\n",
    "\n",
    "\n",
    "# # Set the initial prompt\n",
    "# prompt = \"Once upon\"\n",
    "# input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# # Generate text\n",
    "# output_ids = model.generate(\n",
    "#     input_ids,\n",
    "#     max_length=50,\n",
    "#     logits_processor=logits_processor,\n",
    "#     num_beams=num_beams,  # Adjust for desired trade-off between performance and speed\n",
    "#     eos_token_id=tokenizer.eos_token_id,\n",
    "#     pad_token_id=tokenizer.eos_token_id,\n",
    "# )\n",
    "\n",
    "# # Decode the generated tokens\n",
    "# generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc93cc40-3590-4533-8b4d-2ae4a218897e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tokenizers.Tokenizer' object has no attribute 'convert_tokens_to_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m vocabulary_tokens \u001b[38;5;241m=\u001b[39m vocabulary\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Convert tokens to token IDs\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m vocabulary_token_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m(\u001b[38;5;28mlist\u001b[39m(vocabulary_tokens)))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Include tokens for spaces and punctuation\u001b[39;00m\n\u001b[1;32m     13\u001b[0m space_token_ids \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tokenizers.Tokenizer' object has no attribute 'convert_tokens_to_ids'"
     ]
    }
   ],
   "source": [
    "# # Collect all tokens required to represent the vocabulary words\n",
    "# vocabulary_tokens = set()\n",
    "# for word in vocabulary:\n",
    "#     tokens = tokenizer.tokenize(word)\n",
    "#     vocabulary_tokens.update(tokens)\n",
    "\n",
    "vocabulary_tokens = vocabulary\n",
    "\n",
    "# Convert tokens to token IDs\n",
    "vocabulary_token_ids = set(tokenizer.convert_tokens_to_ids(list(vocabulary_tokens)))\n",
    "\n",
    "# Include tokens for spaces and punctuation\n",
    "space_token_ids = []\n",
    "punctuation_tokens = ['.', ',', '!', '?', ';', ':']\n",
    "\n",
    "for token, token_id in tokenizer.get_vocab().items():\n",
    "    if token.startswith('Ä '):\n",
    "        space_token_ids.append(token_id)\n",
    "    elif token in punctuation_tokens:\n",
    "        space_token_ids.append(token_id)\n",
    "\n",
    "vocabulary_token_ids.update(space_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5482b-b6a7-402c-8294-9b2de1371f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabularyFilterLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, vocabulary_token_ids):\n",
    "        self.vocabulary_token_ids = vocabulary_token_ids\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Create a mask over the vocabulary tokens\n",
    "        vocab_mask = torch.full(scores.shape, False, dtype=torch.bool, device=scores.device)\n",
    "        vocab_mask[..., list(self.vocabulary_token_ids)] = True\n",
    "        # Set scores for tokens not in the vocabulary to negative infinity\n",
    "        scores[..., ~vocab_mask] = -float('inf')\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68196012-94d7-4d82-a53e-9e7a5ad331b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the logits processor with your vocabulary\n",
    "vocabulary_processor = VocabularyFilterLogitsProcessor(vocabulary_token_ids)\n",
    "logits_processor = LogitsProcessorList([vocabulary_processor])\n",
    "\n",
    "# Set the initial prompt\n",
    "prompt = \"Here is a sentence:\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Generate text\n",
    "output_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,\n",
    "    logits_processor=logits_processor,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_k=3,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode the generated tokens\n",
    "generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3561421-b585-4b30-a486-7350fcdc555d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
